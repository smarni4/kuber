########################### Container ######################################

# Container's are isolated environment's with their own operating system, libs, and dependencies.

# As the containers have their own OS, libs, adn dependencies it is easy for operations team to test and build the application.

# Developers will provide an docker image of application that consists of docker file and developed application to the operations team.

# Using that image, the operations team can test and set the application easily because it will run in the same manner on every OS and deploy it easily using the docker file.

########################### Container Orchestration #######################

# After building up the containers, we have to scale up/down based on the users and many containers will base on other containers or other backend services. To run the application without any 
# interruption and to solve these kind of issues, we have to orchestrate the containers by automatically deploying and managing them. This process of automatically deploying and managing them 
# is known as container orchestration.

# Kubernetes is one of the technology that performs container orchestration. There are many technologies such as docker swarm and MESOS, but there are some limitations in docker swarm and MESOS
# is hard to use.

# The advantages of container orchestration are even when your hardware is down, your application will run as it runs on multiple nodes. We can easily deploy nodes in few seconds based on the 
# requirements.

########################### Kubernetes Architecture #######################

# ----> NODES ----> It is a machine, on which kubernetes is installed. They are defined into Master and Worker machines. Containers are launched on nodes using the kubernetes. If a node fails to run the application running on that node will goes down. To overcome this we have to deploy morethan one node.

# ----> CLUSTER --> It is a group of nodes together. If one of the node is failed, the application will run using other nodes. Due to the more number of nodes, the work load is distributed among all the nodes. 

# ----> Master Node --> As I said above, there are two kinds of nodes master and working node. Master node will manage the worker nodes and assign the load to them. If any worker node is failed, the master will assign that failed node work load to other node. It also scales up/down the node count based on the work load.
	
		  ---> Master       ----> Worker 
						
			Kube-apiserver <------> kubelet
			etcd			              Container Runtime
			Controller	            Kube Proxy
			Scheduler

# ----> API server --> It is the pathway between users, management devices, Command line interfaces and kubernetes cluster.

# ----> ETCD ---> It is the key-value store used by the kubernetes to store all data used to manage the cluster. It stores all the data in every node at etcd located in the respective master node. ETCD is responsible for implementing locks within the clusters to ensure there are no conflicts between the masters.

# ----> Controller --> It is respnsible for noticing and responding when nodes, containers, and endpoints goes down. The controllers makes decisions to bring up new nodes, containers in this situation.

# ----> Scheduler ---> It is responsible for scheduling the distribution of containers across nodes.

# ----> Kubelet -----> It is the agent on every node that is responsible to make sure that every container on that node is running as expected. It interacts with the master by forwarding the worker node status to the master and  takes the orders from the master to the worker node.

# ----> Container runtime ---> It is the underlying software that runs the container.( EG: DOCKER )  

######################################################################################
#########################  KUBECTL ###################################################
######################################################################################

 kubectl run hello-minkube -------------------> It deploy the hello-minikube appplication in the kubernetes cluster.

 kubectl cluster-info ------------------------> It shows the information about the cluster.

 kubectl get nodes ---------------------------> It shows all the nodes in the cluster.

 kubectl edit pod [pod_name]  ----------------> It opens the yaml file of pod where we can edit the file and make required changes.

 kubectl apply -f [yamlfile]  ----------------> It creates a pod with the requirements given in that yaml file.

 kubectl create namespace [namespace name] ---> It creates a namespace.

########################## PODS ######################################################

# PODS is the smallest thing in the kubernetes that we can create.

# In kubernetes we deploy applications in the form of containers. However, the containers are not directly deployed into the cluster they will encapsulated into the pods inside the cluster.

# If we want to scale up our application, we have to deploy more number of pods. we shouldn't add the multiple container instances inside single pod to scale up our application.

# A single pod can have multiple instances but not same type of instances. Fr instance, a pod will have a appllication instance and helper container, which will do some supporting task to run  
# the application container successfully.

# These helper container will do the work such as processing the data entered by the user, files uploaded by the user. So, this instance will get created along with the application instance and # get deleted when the application instance is deleted.

# These two instances will communicate with each other directly by reffering to each other as local host since they share the same network namespace and same storage space.

###################################################################################################
###################################################################################################

      To start a new POD --------> kubectl run [POD name] --image [image name] 

      To start the helper POD ---> kubectl run helper -link [pod name]
 
############################# YAML ################################################################

# It represents the config data of the application.

# LISTS 	DICTIONARIES 		LISTS IN DICTIONARIES

LIST NAME:	DICT NAME:		  LIST NAME:
- ITEM		   KEY: VALUE		    - DICT NAME:
- ITEM		   KEY: VALUE			      KEY: VALUE
						 KEY: VALUE
					  LIST NAME2:
					    - DICT NAME2:
                  KEY: VALUE
                  KEY: VALUE

####################### PODS WITH YAML ############################################################

# The kubernetes defination file always consists of four fields: apiVersion, kind, metadate, spec.

# apiVersion: Version of kubernetes api we are using to create the objects. 

# The apiVersion of various objects are: 
	
	POD	 	      v1
	Service  	  v1
	ReplicaSet	apps/v1
	Deployment	apps/v1

# The kind values are POD, Service, Replicaset, Deployment

# The metadata is the data about the object such as name and label.

# The spec is where we provide additional information about our application.

# The example YAML file of the POD is:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:			            ------		
    - name: nginx-container	        |---->	  This is a dictionary with name containers consists of lists starting with name.
      image: nginx 		        ------

# After finishing the YAML file we have to run the command 
        ##### kubectl [create/apply] -f [filename.yml] ##### to create the pod.

# After creating the pod we can verify it by running the commands:

	kubectl get pods	
	
	kubectl describe pod [pod_name]

# To edit the POD info after creating the POD, we have to run the command 
        ##### kubectl edit pod [pod_name] ######

# To execute the commands or debug inside the container, we can log into the container terminal using the command

        ##### kubectl exec -it [pod-name] -- bin/bash       ---> -it = interactive terminal

# To check the logs of the running containers/Pods, we can use the command

        ##### kubectl logs [pod_name]

####################################################################################################################################
############################ Replica Set and Replication Controllers ###############################################################
####################################################################################################################################

####### Replication Controller ########

# The replication controller helps in maintaining the specified number of pods to stay alive inside the node to make the application run without any interruption.

# Using replication controller we can do load balancing and scaling.

# Replication controller and Replica set both have the same purpose but they are not same.

# Replication controller is older version and it is replaced with the Replica set.

# The syntax of relication controller is

apiversion: v1

kind: ReplicationController

metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end

spec:
  template:
    
    metadata:		--------------
      name: myapp-pod		      --
      labels:			            --
	      app: myapp		        --
	      type: front-end		      ---> [ metadata and spec of the pod ]
   				                    --
    spec:			                --
      containers:		          --
	    - name: nginx-container --
	      image: nginx  ----------
  
  replicas:3

# After creating the replication controller yaml file we have to create the controller using the command 
              ### kubectl create/apply -f [filename] #####

# After creating the replication controllers we can cross check them using the command 
          ### kubectl get replicationcontroller ###

# As we mentioned the replicas: 3, the replication controller will create 3 pods automatically. If we delete one of the pod manually or it get deleted the replication controller will
# automatically create the pods untill the number of pods get equal to the replicas mentioned in the yaml file.

############## ReplicaSet ###########

# The replicaset syntax is similar to the replication controller but it has an extra variable known as selector.

# The labels and selectors in yaml file will help the replica set to identify the pods that it has to watch and create one if the number of that pods in the cluster are less than the 
# defined replicas in the replica set yaml file.

# In selector, we have to mention a key ### matchlabels ###, which matches with the labels mentioned in the template dictionary. It helps the replica set to sort out the pods that it 
# has to watch and maintain them from the remaining pods in that cluster.

# The syntax of replicaset is same as the above mentioned syntax of replication controller but the selector key should be added next to the replicas.

  selector:
   matchLabels:
	  type: front-end

# We also need to change the apiversion and kind to apps/v1 and ReplicaSet respectively.

######## Scale ##############33

# To get the replicasets created we have to use the command 
              ### kubectl get replicaset ###

# To delete the replicaset we have to use the command 
              ### kubectl delete replicaset [replicaset name] ###
 
# To scale the replicaset we have to edit the replicaset defination file replicas key value to required number.

# After editing it, we have to replace the file with old file using the command 
              ### kubectl replace -f [filename] ####

# We can also update the file using the command 
            ### kubectl scale --replicas=6 -f [filename] ###
						### kubectl scale --replicas=6 KIND NAME [ given in the metadata ]

# The replicas value in defination file will not get updated if we use the scale command to do so. It only update the system to scale the pods.

# To see the details of replica set we have to use the command
            ### kubectl describe replicaset [replicaset name]
   
#######################################################################################################################################################################################################################DEPLOYMENTS####################################################################################################################################################################################################################################################################

# Deployments will help in deploying multiple sets of replica sets at once. It controls the replica sets and pods inside those replicasets and makes sure that the respectivce number of
# pods and replica sets are running inside it.

# A structure of relation between application container, POD, Replica set, Deployment is 

    Application Container -----> It consists of the application resources.
    POD -----------------------> It consists of Application Container.
    Replica Set ---------------> It consists & manages multiple PODS.
    Deployment ----------------> It consists & manages multiple Replica Sets.

# The structure of deployment yaml file is 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: nginx
    type: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
# After writing a required deployment yaml file we can deploy it using the command 

        kubectl create/apply -f [file-name]

# To replace the file after making some changes in the yaml file 

        kubectl replace -f [file-name]

# To check the existing deployments

        kubectl get deployments

################# Updates and Rollback ##############################################

# When we create a deployment for our application for the first time, it triggers a rollout. A new roll out will creates a new deployment revision.

# When we update an application with some changes, a new deployment revision will be created. It doesn't change the previous deployment revision, so it is easy to go back to the previous 
# version and track our changes.   

# To check our rollout status we can use the command

        kubectl rollout status [deployment-name]

# To check the history of our roll outs

        kubectl rollout history [deployment-name]

# There are two styles of deployments:

    1. Recreate       2. Rolling Update.

# Recreate ---------> In this method, it destroys the present running instances and replaces the updates all at once. So by using this method the application will goes down for certain time
# untill all the updates upload successful.

# Rolling Update ---> In this method, it replaces the updated instances with existing instances one by one to overcome the disadvantage of the recreate method. In this method, the application    # will not goes down while updating.

# By default the upgrade method is rolling Update. If we didn't mention any specific method while deploying the update, it will choose the rolling update by default.

# To do the update, after changing the respective required changes in the deplpoyment yaml file we have to run the following command

          kubectl apply -f [updated-deployment-yaml-file]

# This command will create a new revision to the application.

# We can also update the file using the command

          kubectl set [key-name] [deployment-name] [old-value] = [new-image]

# But the above method is not recommended as it will use the same deployment file. It will be hard to identify the changes in the future.

# To see the details about the updates

          kubectl describe deployment [deployment-name]

################# ROLLBACK ############################

# To rollback to the previous revision [previous_version] 

          kubectl rollout undo [deployment-name]

# To record the command used by us to rollout the instances

          kubectl create -f [file-name] --record=true

# To edit the existing deployment yaml file, we can use the command

          kubectl edit deployment [deployment-name]

###############################################################################################################################################################################################
################################################ SERVICES -PORTS, -CLUSTER IP, -LOAD BALANCER #################################################################################################
###############################################################################################################################################################################################

# Services helps to connect within the application between pods or between the applications or between the data stores and applications.

# It also helps in connecting between backend and frontend of the application.

# In the basic node, assume that there is only one pod in the node and only that single node is present.

# In the node the pod is connected to the cluster cloud with cluster IP address.

# To connect to the appplication inside the pod which is presented inside the node. There are two ways

------> Using SSH, we can enter into the node server IP address and access the web page using the curl command.

------> To access the application from outside the node by using the ip address of the node. We need something to connect between the user, node, and respective pod. This service is known as 
	NODE-PORT service.

############## NODE-PORT service ########################

# In NODE-PORT service, there will be a service which connects the user, node, and pod by mapping them using the ports Target Port, Port, Node Port.

# In this service, there will be a IP address for the service, which is known as cluster IP address.

# Both the Target port and Port will be same, where NodePort ranges betweem 30000 and 32767.

# To create a service, we have to define a YAML file as shown below

--> 	apiVersion: v1
      kind: service
      metadata:
        name: myapp-service
      
      spec:
        type: NodePort
        ports:
        - targetPort: 80
          Port: 80
          nodePort: 30008	     

        selector:
          app: myapp
          type: front-end

# To create the service using the YAML file, we have to run the command

	kubectl create -f [file_name.yml]

# To get the created services

	kubectl get services

# To access the web service creted, we can use the curl command

	curl http://ip-address(Node):[NodePort]

# To map the multiple pods in the single node using the single service, we don't need to do anything ad it will allocate to any of the pod automatically.

# If we need to map the pods which are located in different nodes, the service will also get distributed among those different nodes and maps the respetive pod to the same NodePort. So we can 
# access any pod in any node using the same node port and by changing the ip address respective to the node we are mapping to.

############# CLUSTERIP SERVICES #########################################

# In clusterIP service, the frontend, backend, and  databases or redis are interlinked among them using the services. 

# As the IP addresses are not permanent, we cannot depend on IP address for internal communication.

# To create a cluster IP service,

-----> apiVersion: v1
      kind: Service
      metadata:
        name: back-end
      
      spec:
        type: clusterIP
        ports:
          - targetPort: 80
            port: 80
        
        selector:
          app: myapp
          type: back-end

# To create a service

    kubectl create -f [filename.yaml]

####################################################################################################################################################################################
###################################################### HELM ########################################################################################################################
####################################################################################################################################################################################

# HELM CHARTS --- > The YAML files that are defined previously by someone and packed as a bundle,, which can be used by other developers for their project is knwon as HELM charts.

                    Basically, someone will pack all the basic yaml files required for building an application and make them available for everyone through HELm by uploading them to
                    HELM repository. Anyone who needs to develop an app with specific requirements can easily download them from the respective HELM repository. This group of YAML 
                    files is known as HELM charts.

# Template YAML files ---> The template yaml files is used when we need to create a multiple number of yaml files with same content and minor changes.

                            Basically, template files consists of all the data that is same in all respective files and values that are different in those files are mentioned in 
                            other yaml file sucha s values.yaml and those values are referred in the template file as [ .values.name ].

For example, If we want to create a multiple microservices for an application with same specifications and only difference is their name and image, then we can use the template yaml.

# Template.yaml

apiVersion: v1
kind: pod
metadata:
  name: {{ .values.name }} <----------------|
spec:                                       |  
  containers:                               |
  - name: {{ .Values.container.name }} <----|--|
    image: {{ .Values.container.image }} <--|--|--|
    port: {{ .values.container.port }}  <---|--|--|--|
                                            |  |  |  |
# values.yaml                               |  |  |  | 
                                            |  |  |  |
name: my-app -------------------------------|  |  |  |
container:                                     |  |  |
 name: my-app-container -----------------------|  |  |
 image: my-app-image -----------------------------|  | 
 port: 9001   ---------------------------------------|

# HELM Chart Structure

  Helm chart consists of 
  --> mychart/:   ------------> Chart name.
      -----> chart.yaml ------> meta info about chart.
      -----> values.yaml -----> values for the keys in template files.
      -----> charts/ ---------> This folder consists of chart dependencies.
      -----> templates/ ------> This folder consists of actual template yaml files. 

# When we want to install the template files inside the mychart folder we have to run the command.

        ############# helm install <chart_name> ################

# When we want to override the values in the values.yaml file, we can do it using the command.

        ############# helm install --values=[new_value.yaml] <chart_name> ############

                        or

        ############# helm install --set version=2.0.0 #############
        
# By using the above command we can update the default values or add the new values. 

# When we ran the helm install command, it sends the request to the helm server known as tiller. Tiller gets all the required components and saves a copy of config file  for 
# future references.

# We can upgrade the helm chart files using the command 

        #### helm upgrade <chart_name>

# We can roll back to previous version using the command

        #### helm rollback <chart_name>

# In the recent version of helm the tiller is removed due to the security issues.







